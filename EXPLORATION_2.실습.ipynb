{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94097801",
   "metadata": {},
   "source": [
    "## 이론 정리\n",
    "---\n",
    "> * \"선형 회귀\"에대한 이론 확립  \n",
    "> *  데이터의 분포를 확인하여 인자간 상관관계를 파악하고 상관계수를 구함   \n",
    "> *  1차 함수 모델을 만들며 __가중치__를 배움   \n",
    "     - y = wx + b  \n",
    "> *  현재 모델의 출력값과 실제 값의 차이를 나타내는 __손실함수__를 학습함  \n",
    "     - 평가 지표 4가지 : MAE, MSE, RMSE, R-squared -> RMSE, MSE 사용  \n",
    "> * 기울기를 사용하여 손실함수 값을 점차 줄이는 __경사하강법__을 통하여 모델을 학습시킴  \n",
    "     - w' = w - ng  \n",
    "     -  미분식 이해, 하이퍼 파라미터(컵퓨터 계산입력X, 사용자 직접 입력)  \n",
    "> * 다양한 형태의 그래프를 그릴 수 있는 __seaborn__패키지를 이용하여 다변수 선형회귀를 그림  \n",
    "     - One Hot Encodding : 카테고리 데이터 -> 실수형 데이터 변환 => get_dummies() 함수 (in pandas)    \n",
    "     - 회귀/분류 이해 : 맞추고자하는 값이 카테고리(분류)이냐, 연속된 실수값(회귀)에 따라 다름\n",
    "> * 직접 gradient 값을 계산 및 학습시키기\n",
    "> * __sklearn__라이브러리를 통하여 간단하게 해결  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f28851",
   "metadata": {},
   "source": [
    "## 프로젝트 1 : 당뇨병 수치 계산\n",
    "---\n",
    "\n",
    "### 1) 데이터 가져오기\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "86cd9dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "diabetes = load_diabetes()\n",
    "\n",
    "df_X = diabetes.data\n",
    "df_y = diabetes.target\n",
    "\n",
    "#print(diabetes.DESCR)를 통해 자료 정보학인 \n",
    "#pritn(df.X)를 통해 잘 입력됐는지 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d332f456",
   "metadata": {},
   "source": [
    "- 본 자료는 당뇨병 환자의 데이터로 (442, 10) 크기를 갖고 있음\n",
    "- 총 데이터 갯수는 442개, 특성은 10가지(age, sex, bmi등)가 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a73157f",
   "metadata": {},
   "source": [
    "### 2) 모델에 입력할 데이터 X 준비하기\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "43bc245c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03807591,  0.05068012,  0.06169621, ..., -0.00259226,\n",
       "         0.01990842, -0.01764613],\n",
       "       [-0.00188202, -0.04464164, -0.05147406, ..., -0.03949338,\n",
       "        -0.06832974, -0.09220405],\n",
       "       [ 0.08529891,  0.05068012,  0.04445121, ..., -0.00259226,\n",
       "         0.00286377, -0.02593034],\n",
       "       ...,\n",
       "       [ 0.04170844,  0.05068012, -0.01590626, ..., -0.01107952,\n",
       "        -0.04687948,  0.01549073],\n",
       "       [-0.04547248, -0.04464164,  0.03906215, ...,  0.02655962,\n",
       "         0.04452837, -0.02593034],\n",
       "       [-0.04547248, -0.04464164, -0.0730303 , ..., -0.03949338,\n",
       "        -0.00421986,  0.00306441]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array(df_X)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caebfca6",
   "metadata": {},
   "source": [
    "### 3) 모델에 예측할 데이터 y 준비하기\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a380d172",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([151.,  75., 141., 206., 135.,  97., 138.,  63., 110., 310., 101.,\n",
       "        69., 179., 185., 118., 171., 166., 144.,  97., 168.,  68.,  49.,\n",
       "        68., 245., 184., 202., 137.,  85., 131., 283., 129.,  59., 341.,\n",
       "        87.,  65., 102., 265., 276., 252.,  90., 100.,  55.,  61.,  92.,\n",
       "       259.,  53., 190., 142.,  75., 142., 155., 225.,  59., 104., 182.,\n",
       "       128.,  52.,  37., 170., 170.,  61., 144.,  52., 128.,  71., 163.,\n",
       "       150.,  97., 160., 178.,  48., 270., 202., 111.,  85.,  42., 170.,\n",
       "       200., 252., 113., 143.,  51.,  52., 210.,  65., 141.,  55., 134.,\n",
       "        42., 111.,  98., 164.,  48.,  96.,  90., 162., 150., 279.,  92.,\n",
       "        83., 128., 102., 302., 198.,  95.,  53., 134., 144., 232.,  81.,\n",
       "       104.,  59., 246., 297., 258., 229., 275., 281., 179., 200., 200.,\n",
       "       173., 180.,  84., 121., 161.,  99., 109., 115., 268., 274., 158.,\n",
       "       107.,  83., 103., 272.,  85., 280., 336., 281., 118., 317., 235.,\n",
       "        60., 174., 259., 178., 128.,  96., 126., 288.,  88., 292.,  71.,\n",
       "       197., 186.,  25.,  84.,  96., 195.,  53., 217., 172., 131., 214.,\n",
       "        59.,  70., 220., 268., 152.,  47.,  74., 295., 101., 151., 127.,\n",
       "       237., 225.,  81., 151., 107.,  64., 138., 185., 265., 101., 137.,\n",
       "       143., 141.,  79., 292., 178.,  91., 116.,  86., 122.,  72., 129.,\n",
       "       142.,  90., 158.,  39., 196., 222., 277.,  99., 196., 202., 155.,\n",
       "        77., 191.,  70.,  73.,  49.,  65., 263., 248., 296., 214., 185.,\n",
       "        78.,  93., 252., 150.,  77., 208.,  77., 108., 160.,  53., 220.,\n",
       "       154., 259.,  90., 246., 124.,  67.,  72., 257., 262., 275., 177.,\n",
       "        71.,  47., 187., 125.,  78.,  51., 258., 215., 303., 243.,  91.,\n",
       "       150., 310., 153., 346.,  63.,  89.,  50.,  39., 103., 308., 116.,\n",
       "       145.,  74.,  45., 115., 264.,  87., 202., 127., 182., 241.,  66.,\n",
       "        94., 283.,  64., 102., 200., 265.,  94., 230., 181., 156., 233.,\n",
       "        60., 219.,  80.,  68., 332., 248.,  84., 200.,  55.,  85.,  89.,\n",
       "        31., 129.,  83., 275.,  65., 198., 236., 253., 124.,  44., 172.,\n",
       "       114., 142., 109., 180., 144., 163., 147.,  97., 220., 190., 109.,\n",
       "       191., 122., 230., 242., 248., 249., 192., 131., 237.,  78., 135.,\n",
       "       244., 199., 270., 164.,  72.,  96., 306.,  91., 214.,  95., 216.,\n",
       "       263., 178., 113., 200., 139., 139.,  88., 148.,  88., 243.,  71.,\n",
       "        77., 109., 272.,  60.,  54., 221.,  90., 311., 281., 182., 321.,\n",
       "        58., 262., 206., 233., 242., 123., 167.,  63., 197.,  71., 168.,\n",
       "       140., 217., 121., 235., 245.,  40.,  52., 104., 132.,  88.,  69.,\n",
       "       219.,  72., 201., 110.,  51., 277.,  63., 118.,  69., 273., 258.,\n",
       "        43., 198., 242., 232., 175.,  93., 168., 275., 293., 281.,  72.,\n",
       "       140., 189., 181., 209., 136., 261., 113., 131., 174., 257.,  55.,\n",
       "        84.,  42., 146., 212., 233.,  91., 111., 152., 120.,  67., 310.,\n",
       "        94., 183.,  66., 173.,  72.,  49.,  64.,  48., 178., 104., 132.,\n",
       "       220.,  57.])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array(df_y)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d35e1b3",
   "metadata": {},
   "source": [
    "### 4) train 데이터와 test 데이터로 분리하기\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5f26fcf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(353, 10) (353,)\n",
      "(89, 10) (89,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=50)\n",
    "\n",
    "#잘 나눠졌는지 확인\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112864b1",
   "metadata": {},
   "source": [
    "### 5) 모델 준비하기\n",
    "---  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "94a04c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 데이터 갯수에 맞는 가중치 w, b 준비하기\n",
    "w = np.random.rand(10)\n",
    "b = np.random.rand()\n",
    "\n",
    "#모델 함수 구현하기\n",
    "#for문을 활용하여 y = w1x1 + w2x2 + ~ + w10x10 + b 식 만들기\n",
    "def model(X, w, b):\n",
    "    predictions = 0\n",
    "    for i in range(10):\n",
    "        predictions += X[:, i] * w[i]\n",
    "    predictions += b\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50db9c20",
   "metadata": {},
   "source": [
    "### 6) 손실함수 loss 정의하기 \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "8adf0733",
   "metadata": {},
   "outputs": [],
   "source": [
    "#손실함수를 MSE 함수로 정의하기 \n",
    "def MSE(a, b):\n",
    "    mse = ((a - b) ** 2).mean()\n",
    "    return mse\n",
    "\n",
    "def loss(X, w, b, y):\n",
    "    predictions = model(X, w, b)\n",
    "    L = MSE(predictions, y)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ded3b43",
   "metadata": {},
   "source": [
    "### 7) 기울기를 구하는 gradient 함수 구현하기\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a04640e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -60.60401889  -13.74495062 -189.52307941 -142.65567638  -68.31156773\n",
      "  -56.06131976  127.64376368 -139.02866396 -182.82701013 -123.47748536]\n",
      "-302.89719317467757\n"
     ]
    }
   ],
   "source": [
    "def gradient(X, w, b, y):\n",
    "    N = len(w)\n",
    "    \n",
    "    #y_pred 준비\n",
    "    y_pred = model(X, w, b)\n",
    "    \n",
    "    #w의 gradient 식-> **X.dot(y)는 X와 y의 행렬곱. T는 형태 바꿔줌 \n",
    "    dw = 1/N * 2 * X.T.dot(y_pred - y)\n",
    "    \n",
    "    #b의 gradient 식 \n",
    "    db = 2 * (y_pred - y).mean()\n",
    "    \n",
    "    return dw, db\n",
    "\n",
    "#시험삼아 계산\n",
    "dw, db = gradient(X, w, b, y)\n",
    "print(dw)\n",
    "print(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bd13e0",
   "metadata": {},
   "source": [
    "### 8) 하이퍼 파라미터인 학습률 설정하기\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3db577e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning rate 설정하기!\n",
    "learning_rate = 0.1\n",
    "\n",
    "#1: 5 넘 높음.  2: 100 Loss nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c4dbd6",
   "metadata": {},
   "source": [
    "### 9) 모델 학습하기\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "bc16ce5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 50 : Loss nan\n",
      "iteration 100 : Loss nan\n",
      "iteration 150 : Loss nan\n",
      "iteration 200 : Loss nan\n",
      "iteration 250 : Loss nan\n",
      "iteration 300 : Loss nan\n",
      "iteration 350 : Loss nan\n",
      "iteration 400 : Loss nan\n",
      "iteration 450 : Loss nan\n",
      "iteration 500 : Loss nan\n",
      "iteration 550 : Loss nan\n",
      "iteration 600 : Loss nan\n",
      "iteration 650 : Loss nan\n",
      "iteration 700 : Loss nan\n",
      "iteration 750 : Loss nan\n",
      "iteration 800 : Loss nan\n",
      "iteration 850 : Loss nan\n",
      "iteration 900 : Loss nan\n",
      "iteration 950 : Loss nan\n"
     ]
    }
   ],
   "source": [
    "#정해진 손실함수와 기울기 함수로 모델 학습시키기\n",
    "losses = []\n",
    "\n",
    "for i in range(1,1000):\n",
    "    dw, db = gradient(X_train, w, b, y_train)\n",
    "    w -= learning_rate * dw\n",
    "    b -= learning_rate * db\n",
    "    L = loss(X_train, w, b, y_train)\n",
    "    losses.append(L)\n",
    "    \n",
    "    if i % 50 == 0:\n",
    "            print('iteration %d : Loss %0.4f' % (i, L) )\n",
    "\n",
    "\n",
    "#자꾸 loss nan 뜸...나니....?\n",
    "#L, dw, db 값이 nan으로 나옴. -> test의 수 바꿔줌. -> lss값 20000~15000 \n",
    "#-> lr 0.001 -> 0.0.1 => 6000~3000 -> 1로하니까 2000대"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afac934",
   "metadata": {},
   "source": [
    "ndarray 구현방식  \n",
    "\n",
    "ndarray는 일반 파이썬 List의 구현방식(Linked List)와 다르게 C의 배열(array)의 특성인 연속적인 메모리에 배치된다는 점입니다.  \n",
    "\n",
    "이로 인해 C의 array가 가지는 장점은 살리면서 파이썬의 직관적인 코딩도 가능하게 됩니다.  \n",
    "\n",
    "인접한 메모리 배치는 다수의 선형대수 연산의 속도를 향상시킬 수 있습니다.  \n",
    "\n",
    "그래서 전문가들의 python 성능 향상을 위한 코딩 관례중 python에서 ndarray의 벡터화 연산으로 계산할 수 있는 경우의 파이썬 내장 반복문은 사용하지 않는다는 점이라고 합니다.\n",
    "\n",
    "\n",
    "Numerical Exception\n",
    "수학적으로 계산이 안되는 것. 0으로 나눈다거나 log(0)과 같은 +-무한대로 발산할 수 있는 것이 생기는 경우\n",
    "\n",
    "Nan =  NaN은 Not-a-Number 로 숫자가 아니라는 뜻"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a4ad3d",
   "metadata": {},
   "source": [
    "/tmp/ipykernel_48/2843987338.py:3: RuntimeWarning: overflow encountered in square\n",
    "  mse = ((a - b) ** 2).mean()\n",
    "/tmp/ipykernel_48/2118051103.py:7: RuntimeWarning: invalid value encountered in double_scalars\n",
    "  b -= learning_rate * db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd3966a",
   "metadata": {},
   "source": [
    "(9) 모델 학습하기\n",
    "정의된 손실함수와 기울기 함수로 모델을 학습해주세요.\n",
    "loss값이 충분히 떨어질 때까지 학습을 진행해주세요.\n",
    "입력하는 데이터인 X에 들어가는 특성 컬럼들을 몇 개 빼도 괜찮습니다. 다양한 데이터로 실험해 보세요.\n",
    "(10) test 데이터에 대한 성능 확인하기\n",
    "test 데이터에 대한 성능을 확인해주세요.\n",
    "(11) 정답 데이터와 예측한 데이터 시각화하기\n",
    "x축에는 X 데이터의 첫 번째 컬럼을, y축에는 정답인 target 데이터를 넣어서 모델이 예측한 데이터를 시각화해 주세요."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
